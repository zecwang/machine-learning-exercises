{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<h2>INFSCI 2595 Machine Learning - Fall 2018 </h2>\n",
    "<h1 style=\"font-size: 250%;\">Assignment #2</h1>\n",
    "<h3>Total points: 100 </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type in your information in the double quotes\n",
    "firstName = \"\"\n",
    "lastName = \"\"\n",
    "pittID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <h3>  Problem #1. Linear Discriminant Analysis (LDA)and Quadratic Discriminant Analysis(QDA) </h3> \n",
    " ### [30 points]\n",
    " \n",
    "Do not write a code for this part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-1</h4>  <br>\n",
    "Assume we have K classes to be classified with one feature $(x)$. The prior probability of\n",
    "class $k$ is $ùúã_{k} = ùëÉ(ùëå = ùëò)$. Assume that the feature in class k has Gaussian distribution of\n",
    "mean $Œº_{k}$ and variance $œÉ^2 (ùí©(Œº,ùúé^{2}))$.The variance is the same for all classes. \n",
    "Prove that the Bayes‚Äô classifier (that chooses class k with largest $ùëÉ(ùëå = ùëò|ùë•))$ is equivalent to assigning an observation to the class for which the discriminant function $ùõø_{k}(x)$ is\n",
    "maximized, where \n",
    "\\begin{array} \\\\\n",
    "ùõø_{k}(x) = x\\frac{\\mu _{k}}{\\sigma ^{2}}- \\frac{\\mu_{2}^{k}}{2\\sigma ^{2}}+ log(\\pi _{k})\n",
    "\\end{array}\n",
    "<br> What is the name of this classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Answer</h2><br>\n",
    "Bayes rule is:\n",
    "\\begin{array} \\\\\n",
    "P(Y = k|x) = \\frac{P(Y = X|k)P(Y=k)}{P(X=x)}\n",
    "\\end{array}\n",
    "\n",
    "For this we know that : \n",
    "\\begin{array} \\\\\n",
    "P(X= x) = \\Sigma_{i=1}^{K}\\pi_i f_i(x)\\\\\n",
    "P(Y = x|k)P(Y =k)= f_k(x)\\pi_k = \\frac{1}{\\sqrt(2\\pi\\sigma)}\\exp(-\\frac{(x-\\mu_k)^2}{2\\sigma^2})\\pi_k\n",
    "\\end{array}\n",
    "\n",
    "Now we need to take a logs\n",
    "\n",
    "\\begin{array} \\\\\n",
    "P(Y = x|k)P(Y =k)=  log({-\\sqrt(2\\pi\\sigma)}) +log(\\pi_k) + (-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma})^2)\\\\\n",
    "P(Y = x|k)P(Y =k)=  log(\\pi_k)+\\frac{x\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2} {2\\sigma^2}\\\\\n",
    "P(Y = x|k)P(Y =k)= ùõø_{k}(x)\n",
    "\\end{array}\n",
    "\n",
    "<br> \n",
    "- **What is the name of this classifier?**\n",
    "    - The name of this classifier is: ** linear Discriminant Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-2</h4>  <br>\n",
    "Extend **Problem #1-1** to include **p** features. With features from each class drawn from a\n",
    "Gaussian distribution with mean vector $Œº_{k}$ and covariance matrix $Œ£_{k}$ (which is now\n",
    "different for each class). What is the discriminant function that maximizes **ùëÉ(ùëå = ùëò|ùë•)**. Is\n",
    "the relationship with the feature vector **x** linear?<br> What is this classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Answer</h2><br>\n",
    "By following the same procedure as above, without ignoring the variance term, you can proof that the discriminant function is given by:\n",
    "\n",
    "\n",
    "First eqiasion is :\n",
    "\\begin{array} \\\\\n",
    "\\sigma_{k}(x) = \\log{(\\pi_k)} - \\frac{1}{2}\\log{|\\Sigma_{k}|}+ x^{T}\\Sigma_{k}^{-1}\\mu_{k} - \\frac{1}{2}x^{T}\\Sigma_{k}^{-1}x - \\frac{1}{2}\\mu_{k}^{T}\\Sigma_{k}^{-1}\\mu_{k} \n",
    "\\end{array}\n",
    "\n",
    "\n",
    "Finding discriminant function that maximizes **ùëÉ(ùëå = ùëò|ùë•)**\n",
    "\\begin{array} \\\\\n",
    "P(Y = k|x) = \\log{[(-2 \\pi)^{\\frac{p}{2}}|\\Sigma_{k}|^{Y_{2}}]} + \\exp[- \\frac{1}{2} (x-\\mu_{k})^{T}\\Sigma_{k}^{-1}(x-\\mu_{k}) ] +\\log{(\\pi_k)}\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array} \\\\\n",
    "P(Y = k|x) =  \\frac{1}{2}\\log{(|\\Sigma_{k}|)} + +\\log{(\\pi_k)} - \\frac{1}{2}(x^{T}\\Sigma_{k}^{-1}x) +\\frac{1}{2}(x^{T}\\Sigma_{k}^{-1}\\mu_{k}) + \\frac{1}{2}(\\mu_{k}^{T}\\Sigma_{k}^{-1}x) \\frac{1}{2}(\\mu_{k}^{T}\\Sigma_{k}^{-1}\\mu_{k})\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array} \\\\\n",
    "P(Y = k|x) =  \\frac{1}{2}\\log{(|\\Sigma_{k}|)} +\\log{(\\pi_k)} - \\frac{1}{2}x^{T}\\Sigma_{k}^{-1}x +x^{T}\\Sigma_{k}^{-1}\\mu_{k}-\\frac{1}{2}\\mu_{k}^{T}\\Sigma_{k}^{-1}\\mu_{k} = \\sigma_{k}(x)\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array} \\\\\n",
    "P(Y = k|x)  = \\sigma_{k}(x)\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "<br> \n",
    "- **What is the name of this classifier?**\n",
    "    - The name of this classifier is: ** Quadratic Discriminant Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-3</h4>  <br>\n",
    "- Explain Bias-variance trade-off in choosing between LDA and QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Answer</h2>\n",
    "- LDA assumed the equal variance for all classes while QDA assumes different covariance for each class. Hence, for LDA the number of parameters that need to be estimated is less than that for QDA. \n",
    "- If the number of training observations is small, QDA may suffer from high variance (overfit) and low bias. On the other hand, LDA may suffer from high bias, and low variance, especially if the actual dataset is not linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <h3>  Problem #2. Regularization    </h3>\n",
    "### [15 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #2-1. </h4> Answer the following questions \n",
    "\n",
    "- What is a purpose using regularization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4> Answer:</h4>\n",
    "\n",
    "- To solve/undermine overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #2-2.Logistic Regression with Ridge Regularization (part 2) </h4>  <br>\n",
    "\n",
    "In this part, you should download and analyze the Wisconson **\"breast_cancer\"** dataset. <br>\n",
    "\n",
    "- Fit logistic regression model using ridge regularization with different values of  C = 0.1, 1, 5, 10, 50, 100, and 1000 (Note that C is the LogisticRegression function argument). For each value, report the estimated coefficients for the fitted model (do not just print summary, make a table with feature names and estimated coefficients)\n",
    "\n",
    "- What happens to the coefficients as you increase C?\n",
    "\n",
    "- What happens to the flexibility of the model as you increase C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "dataset = load_breast_cancer()\n",
    "#print(dataset.DESCR)\n",
    "DataFrame = pd.DataFrame(dataset.data, columns= dataset.feature_names) # tranform it into dataframe for further convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(dataset.data, dataset.target, random_state= 0)\n",
    "\n",
    "# fit linear regression model, each model has different C values.\n",
    "# 'l2' penalty means Ridge regularization\n",
    "RModel01=LogisticRegression(C=0.1, penalty='l2').fit(X_train, Y_train)\n",
    "RModel1=LogisticRegression(C=1, penalty='l2').fit(X_train, Y_train)\n",
    "RModel5=LogisticRegression(C=5, penalty='l2').fit(X_train, Y_train)\n",
    "RModel10=LogisticRegression(C=10, penalty='l2').fit(X_train, Y_train)\n",
    "RModel50=LogisticRegression(C=50, penalty='l2').fit(X_train, Y_train)\n",
    "RModel100=LogisticRegression(C=100, penalty='l2').fit(X_train, Y_train)\n",
    "RModel1000=LogisticRegression(C=1000, penalty='l2').fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature names</th>\n",
       "      <th>Rigde C = 0.1</th>\n",
       "      <th>Rigde C = 1</th>\n",
       "      <th>Rigde C = 5</th>\n",
       "      <th>Rigde C = 10</th>\n",
       "      <th>Rigde C = 50</th>\n",
       "      <th>Rigde C = 100</th>\n",
       "      <th>Rigde C = 1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean radius</td>\n",
       "      <td>0.5354364530378037</td>\n",
       "      <td>1.722136811695048</td>\n",
       "      <td>2.5344095331174716</td>\n",
       "      <td>3.0582554454020827</td>\n",
       "      <td>3.2747182941450497</td>\n",
       "      <td>3.856342243941693</td>\n",
       "      <td>3.963610586011696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean texture</td>\n",
       "      <td>0.07679211361324033</td>\n",
       "      <td>0.08980850223340164</td>\n",
       "      <td>0.09848848019210252</td>\n",
       "      <td>0.12590852872981056</td>\n",
       "      <td>0.10807448519514841</td>\n",
       "      <td>0.13117844737261428</td>\n",
       "      <td>0.13047814652008993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean perimeter</td>\n",
       "      <td>0.4037953685460963</td>\n",
       "      <td>0.10599595053300613</td>\n",
       "      <td>-0.09813535783139571</td>\n",
       "      <td>-0.10143247760131299</td>\n",
       "      <td>-0.23447235438095199</td>\n",
       "      <td>-0.2950483797402432</td>\n",
       "      <td>-0.305155401605373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mean area</td>\n",
       "      <td>-0.012595806561600847</td>\n",
       "      <td>-0.0071349526416984764</td>\n",
       "      <td>-0.0017852218886192503</td>\n",
       "      <td>-0.011096328717021235</td>\n",
       "      <td>-0.005093103011806296</td>\n",
       "      <td>-0.006355538472142261</td>\n",
       "      <td>-0.007379317303412497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean smoothness</td>\n",
       "      <td>-0.020001812520736396</td>\n",
       "      <td>-0.12840859067084384</td>\n",
       "      <td>-0.19217669085131725</td>\n",
       "      <td>-0.7382239701774852</td>\n",
       "      <td>-2.4932570842956583</td>\n",
       "      <td>-1.4293813381225469</td>\n",
       "      <td>-1.61712781522583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mean compactness</td>\n",
       "      <td>-0.08644006157313153</td>\n",
       "      <td>-0.33349469034569296</td>\n",
       "      <td>-0.5001534937214936</td>\n",
       "      <td>-0.5604633939519462</td>\n",
       "      <td>-0.08266529548282406</td>\n",
       "      <td>-0.6250738727836207</td>\n",
       "      <td>-0.6209264802062653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mean concavity</td>\n",
       "      <td>-0.12275348168553382</td>\n",
       "      <td>-0.4993559261747202</td>\n",
       "      <td>-0.7465486507799042</td>\n",
       "      <td>-1.093016193608723</td>\n",
       "      <td>-1.612132860893828</td>\n",
       "      <td>-1.3811669266302544</td>\n",
       "      <td>-1.4323139326780026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mean concave points</td>\n",
       "      <td>-0.05363101921559432</td>\n",
       "      <td>-0.2650783573361579</td>\n",
       "      <td>-0.39536089937655444</td>\n",
       "      <td>-1.2388532447256428</td>\n",
       "      <td>-4.145678723040744</td>\n",
       "      <td>-2.331816097990524</td>\n",
       "      <td>-2.625528518357439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean symmetry</td>\n",
       "      <td>-0.03981892759441688</td>\n",
       "      <td>-0.26760588967877075</td>\n",
       "      <td>-0.3990040364338749</td>\n",
       "      <td>-1.5601909422265985</td>\n",
       "      <td>-4.721807060261473</td>\n",
       "      <td>-2.9437970370641824</td>\n",
       "      <td>-3.316452130495724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mean fractal dimension</td>\n",
       "      <td>-0.004791355054251155</td>\n",
       "      <td>-0.021494610890731916</td>\n",
       "      <td>-0.032546870571283325</td>\n",
       "      <td>0.0474677972454204</td>\n",
       "      <td>0.4827720188994957</td>\n",
       "      <td>0.1728835547577065</td>\n",
       "      <td>0.20988582610340395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>radius error</td>\n",
       "      <td>0.022073040507326412</td>\n",
       "      <td>0.03647246929594611</td>\n",
       "      <td>0.05924567497822165</td>\n",
       "      <td>0.1347783293816018</td>\n",
       "      <td>0.9315498474800399</td>\n",
       "      <td>0.36136165736459636</td>\n",
       "      <td>0.421144081923829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>texture error</td>\n",
       "      <td>0.20568262230427933</td>\n",
       "      <td>0.9865291006520601</td>\n",
       "      <td>1.5052451816420649</td>\n",
       "      <td>1.7625302270402532</td>\n",
       "      <td>2.008441148439241</td>\n",
       "      <td>2.090227255460573</td>\n",
       "      <td>2.068811281787713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>perimeter error</td>\n",
       "      <td>0.035415515383543456</td>\n",
       "      <td>0.11708896135867208</td>\n",
       "      <td>0.23624149320454668</td>\n",
       "      <td>0.1442113136885499</td>\n",
       "      <td>-0.21415852750297731</td>\n",
       "      <td>-0.5763351458980054</td>\n",
       "      <td>-0.5909228390625765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>area error</td>\n",
       "      <td>-0.08275605170360438</td>\n",
       "      <td>-0.10871692288104907</td>\n",
       "      <td>-0.13312362720808862</td>\n",
       "      <td>-0.13412157503569638</td>\n",
       "      <td>-0.13114367044371567</td>\n",
       "      <td>-0.09860368260976583</td>\n",
       "      <td>-0.09788001192412503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>smoothness error</td>\n",
       "      <td>-0.0012682970096307901</td>\n",
       "      <td>-0.00796625721217842</td>\n",
       "      <td>-0.011940339973724986</td>\n",
       "      <td>-0.05482624964728586</td>\n",
       "      <td>-0.1929450247226859</td>\n",
       "      <td>-0.10887757276342173</td>\n",
       "      <td>-0.1238872096313084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>compactness error</td>\n",
       "      <td>-0.013736138508849145</td>\n",
       "      <td>0.010562966275331515</td>\n",
       "      <td>0.014010946098933564</td>\n",
       "      <td>0.8225870339101291</td>\n",
       "      <td>4.080298770380632</td>\n",
       "      <td>1.9184307012676023</td>\n",
       "      <td>2.2277746399679694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>concavity error</td>\n",
       "      <td>-0.02231784235798147</td>\n",
       "      <td>-0.029184939767739865</td>\n",
       "      <td>-0.04486375148545969</td>\n",
       "      <td>0.9147958672032822</td>\n",
       "      <td>4.9094721686128215</td>\n",
       "      <td>2.3800126032280926</td>\n",
       "      <td>2.7974068929521785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>concave points error</td>\n",
       "      <td>-0.006675527857979183</td>\n",
       "      <td>-0.028182556317611886</td>\n",
       "      <td>-0.041959281086823036</td>\n",
       "      <td>-0.07559086119869528</td>\n",
       "      <td>-0.15473413033936637</td>\n",
       "      <td>-0.10324063794132834</td>\n",
       "      <td>-0.10904409434518669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>symmetry error</td>\n",
       "      <td>-0.006715929397405169</td>\n",
       "      <td>-0.034313269666766906</td>\n",
       "      <td>-0.05087227636567235</td>\n",
       "      <td>-0.02519458079134028</td>\n",
       "      <td>0.5351243899056088</td>\n",
       "      <td>0.069028700450717</td>\n",
       "      <td>0.09757457423900787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fractal dimension error</td>\n",
       "      <td>-0.0008073122948884585</td>\n",
       "      <td>0.00856817555358149</td>\n",
       "      <td>0.012541191517373854</td>\n",
       "      <td>0.16954501454675922</td>\n",
       "      <td>0.7873161185959036</td>\n",
       "      <td>0.38071830796644357</td>\n",
       "      <td>0.4394789383176718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>worst radius</td>\n",
       "      <td>0.5552686045830728</td>\n",
       "      <td>1.3583093424401862</td>\n",
       "      <td>2.0204777494566115</td>\n",
       "      <td>1.1389886654256645</td>\n",
       "      <td>1.158537200557793</td>\n",
       "      <td>1.0455291300822682</td>\n",
       "      <td>0.8779733237284706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>worst texture</td>\n",
       "      <td>-0.20479152061466907</td>\n",
       "      <td>-0.28903992036567167</td>\n",
       "      <td>-0.35060532350852974</td>\n",
       "      <td>-0.3879531712747891</td>\n",
       "      <td>-0.40557116871306914</td>\n",
       "      <td>-0.43330531193157845</td>\n",
       "      <td>-0.4309817890438629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>worst perimeter</td>\n",
       "      <td>-0.2603235643907413</td>\n",
       "      <td>-0.24983561163683993</td>\n",
       "      <td>-0.2502399082196183</td>\n",
       "      <td>-0.16756113339943357</td>\n",
       "      <td>-0.0872184031488178</td>\n",
       "      <td>-0.04707850935548225</td>\n",
       "      <td>-0.02932116411764707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>worst area</td>\n",
       "      <td>-0.014497284538983532</td>\n",
       "      <td>-0.020123836123508057</td>\n",
       "      <td>-0.026347591063339316</td>\n",
       "      <td>-0.020019733893304803</td>\n",
       "      <td>-0.024589444804098384</td>\n",
       "      <td>-0.027948349105115566</td>\n",
       "      <td>-0.026950120141786875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>worst smoothness</td>\n",
       "      <td>-0.03607865649468852</td>\n",
       "      <td>-0.21696376476471999</td>\n",
       "      <td>-0.324442408935229</td>\n",
       "      <td>-1.2067075920610877</td>\n",
       "      <td>-4.0483086639823025</td>\n",
       "      <td>-2.3277105606485584</td>\n",
       "      <td>-2.6334394203545397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>worst compactness</td>\n",
       "      <td>-0.27357381233472944</td>\n",
       "      <td>-1.0274530578243533</td>\n",
       "      <td>-1.5462984230329597</td>\n",
       "      <td>-1.5994022642058932</td>\n",
       "      <td>0.007327193877736401</td>\n",
       "      <td>-1.673176679725697</td>\n",
       "      <td>-1.6238676066717739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>worst concavity</td>\n",
       "      <td>-0.35806889161254196</td>\n",
       "      <td>-1.4479418836023858</td>\n",
       "      <td>-2.169177717983175</td>\n",
       "      <td>-2.9088214552182987</td>\n",
       "      <td>-3.5085407850251227</td>\n",
       "      <td>-3.2121692235197026</td>\n",
       "      <td>-3.1893151372748214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>worst concave points</td>\n",
       "      <td>-0.10869076018787718</td>\n",
       "      <td>-0.5337734803028379</td>\n",
       "      <td>-0.794860013016279</td>\n",
       "      <td>-2.4245216714054227</td>\n",
       "      <td>-7.930381440490669</td>\n",
       "      <td>-4.488536600788137</td>\n",
       "      <td>-5.040184745679107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>worst symmetry</td>\n",
       "      <td>-0.10237073852617422</td>\n",
       "      <td>-0.6485577738352347</td>\n",
       "      <td>-0.9643637748156108</td>\n",
       "      <td>-2.907652528600781</td>\n",
       "      <td>-6.045797881042173</td>\n",
       "      <td>-4.923043127447918</td>\n",
       "      <td>-5.4476932894000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>worst fractal dimension</td>\n",
       "      <td>-0.026369797182942127</td>\n",
       "      <td>-0.10913287129271841</td>\n",
       "      <td>-0.1647832050593445</td>\n",
       "      <td>-0.13050273747442775</td>\n",
       "      <td>0.28556501827531566</td>\n",
       "      <td>-0.060629331169069246</td>\n",
       "      <td>-0.03345700886316364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Feature names          Rigde C = 0.1              Rigde C = 1  \\\n",
       "0               mean radius      0.5354364530378037       1.722136811695048   \n",
       "1              mean texture     0.07679211361324033     0.08980850223340164   \n",
       "2            mean perimeter      0.4037953685460963     0.10599595053300613   \n",
       "3                 mean area   -0.012595806561600847  -0.0071349526416984764   \n",
       "4           mean smoothness   -0.020001812520736396    -0.12840859067084384   \n",
       "5          mean compactness    -0.08644006157313153    -0.33349469034569296   \n",
       "6            mean concavity    -0.12275348168553382     -0.4993559261747202   \n",
       "7       mean concave points    -0.05363101921559432     -0.2650783573361579   \n",
       "8             mean symmetry    -0.03981892759441688    -0.26760588967877075   \n",
       "9    mean fractal dimension   -0.004791355054251155   -0.021494610890731916   \n",
       "10             radius error    0.022073040507326412     0.03647246929594611   \n",
       "11            texture error     0.20568262230427933      0.9865291006520601   \n",
       "12          perimeter error    0.035415515383543456     0.11708896135867208   \n",
       "13               area error    -0.08275605170360438    -0.10871692288104907   \n",
       "14         smoothness error  -0.0012682970096307901    -0.00796625721217842   \n",
       "15        compactness error   -0.013736138508849145    0.010562966275331515   \n",
       "16          concavity error    -0.02231784235798147   -0.029184939767739865   \n",
       "17     concave points error   -0.006675527857979183   -0.028182556317611886   \n",
       "18           symmetry error   -0.006715929397405169   -0.034313269666766906   \n",
       "19  fractal dimension error  -0.0008073122948884585     0.00856817555358149   \n",
       "20             worst radius      0.5552686045830728      1.3583093424401862   \n",
       "21            worst texture    -0.20479152061466907    -0.28903992036567167   \n",
       "22          worst perimeter     -0.2603235643907413    -0.24983561163683993   \n",
       "23               worst area   -0.014497284538983532   -0.020123836123508057   \n",
       "24         worst smoothness    -0.03607865649468852    -0.21696376476471999   \n",
       "25        worst compactness    -0.27357381233472944     -1.0274530578243533   \n",
       "26          worst concavity    -0.35806889161254196     -1.4479418836023858   \n",
       "27     worst concave points    -0.10869076018787718     -0.5337734803028379   \n",
       "28           worst symmetry    -0.10237073852617422     -0.6485577738352347   \n",
       "29  worst fractal dimension   -0.026369797182942127    -0.10913287129271841   \n",
       "\n",
       "              Rigde C = 5            Rigde C = 10          Rigde C = 50   \\\n",
       "0       2.5344095331174716     3.0582554454020827     3.2747182941450497   \n",
       "1      0.09848848019210252    0.12590852872981056    0.10807448519514841   \n",
       "2     -0.09813535783139571   -0.10143247760131299   -0.23447235438095199   \n",
       "3   -0.0017852218886192503  -0.011096328717021235  -0.005093103011806296   \n",
       "4     -0.19217669085131725    -0.7382239701774852    -2.4932570842956583   \n",
       "5      -0.5001534937214936    -0.5604633939519462   -0.08266529548282406   \n",
       "6      -0.7465486507799042     -1.093016193608723     -1.612132860893828   \n",
       "7     -0.39536089937655444    -1.2388532447256428     -4.145678723040744   \n",
       "8      -0.3990040364338749    -1.5601909422265985     -4.721807060261473   \n",
       "9    -0.032546870571283325     0.0474677972454204     0.4827720188994957   \n",
       "10     0.05924567497822165     0.1347783293816018     0.9315498474800399   \n",
       "11      1.5052451816420649     1.7625302270402532      2.008441148439241   \n",
       "12     0.23624149320454668     0.1442113136885499   -0.21415852750297731   \n",
       "13    -0.13312362720808862   -0.13412157503569638   -0.13114367044371567   \n",
       "14   -0.011940339973724986   -0.05482624964728586    -0.1929450247226859   \n",
       "15    0.014010946098933564     0.8225870339101291      4.080298770380632   \n",
       "16    -0.04486375148545969     0.9147958672032822     4.9094721686128215   \n",
       "17   -0.041959281086823036   -0.07559086119869528   -0.15473413033936637   \n",
       "18    -0.05087227636567235   -0.02519458079134028     0.5351243899056088   \n",
       "19    0.012541191517373854    0.16954501454675922     0.7873161185959036   \n",
       "20      2.0204777494566115     1.1389886654256645      1.158537200557793   \n",
       "21    -0.35060532350852974    -0.3879531712747891   -0.40557116871306914   \n",
       "22     -0.2502399082196183   -0.16756113339943357    -0.0872184031488178   \n",
       "23   -0.026347591063339316  -0.020019733893304803  -0.024589444804098384   \n",
       "24      -0.324442408935229    -1.2067075920610877    -4.0483086639823025   \n",
       "25     -1.5462984230329597    -1.5994022642058932   0.007327193877736401   \n",
       "26      -2.169177717983175    -2.9088214552182987    -3.5085407850251227   \n",
       "27      -0.794860013016279    -2.4245216714054227     -7.930381440490669   \n",
       "28     -0.9643637748156108     -2.907652528600781     -6.045797881042173   \n",
       "29     -0.1647832050593445   -0.13050273747442775    0.28556501827531566   \n",
       "\n",
       "            Rigde C = 100         Rigde C = 1000  \n",
       "0       3.856342243941693      3.963610586011696  \n",
       "1     0.13117844737261428    0.13047814652008993  \n",
       "2     -0.2950483797402432     -0.305155401605373  \n",
       "3   -0.006355538472142261  -0.007379317303412497  \n",
       "4     -1.4293813381225469      -1.61712781522583  \n",
       "5     -0.6250738727836207    -0.6209264802062653  \n",
       "6     -1.3811669266302544    -1.4323139326780026  \n",
       "7      -2.331816097990524     -2.625528518357439  \n",
       "8     -2.9437970370641824     -3.316452130495724  \n",
       "9      0.1728835547577065    0.20988582610340395  \n",
       "10    0.36136165736459636      0.421144081923829  \n",
       "11      2.090227255460573      2.068811281787713  \n",
       "12    -0.5763351458980054    -0.5909228390625765  \n",
       "13   -0.09860368260976583   -0.09788001192412503  \n",
       "14   -0.10887757276342173    -0.1238872096313084  \n",
       "15     1.9184307012676023     2.2277746399679694  \n",
       "16     2.3800126032280926     2.7974068929521785  \n",
       "17   -0.10324063794132834   -0.10904409434518669  \n",
       "18      0.069028700450717    0.09757457423900787  \n",
       "19    0.38071830796644357     0.4394789383176718  \n",
       "20     1.0455291300822682     0.8779733237284706  \n",
       "21   -0.43330531193157845    -0.4309817890438629  \n",
       "22   -0.04707850935548225   -0.02932116411764707  \n",
       "23  -0.027948349105115566  -0.026950120141786875  \n",
       "24    -2.3277105606485584    -2.6334394203545397  \n",
       "25     -1.673176679725697    -1.6238676066717739  \n",
       "26    -3.2121692235197026    -3.1893151372748214  \n",
       "27     -4.488536600788137     -5.040184745679107  \n",
       "28     -4.923043127447918    -5.4476932894000285  \n",
       "29  -0.060629331169069246   -0.03345700886316364  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we concatenate the coefficents of different models (based on C values) into a dataframe.\n",
    "# We concatenate them sequentially.\n",
    "# .coef_ is used to obtain the coefficients from a model.\n",
    "x1 = np.concatenate((dataset.feature_names.reshape(-1,1),RModel01.coef_[:,None][0][0].reshape(-1,1)),axis=1)\n",
    "x2 = np.concatenate((x1,RModel1.coef_[:,None].reshape(-1,1)),axis=1)\n",
    "x3 = np.concatenate((x2,RModel5.coef_[:,None].reshape(-1,1)),axis=1)\n",
    "x4 = np.concatenate((x3,RModel10.coef_[:,None].reshape(-1,1)),axis=1)\n",
    "x5 = np.concatenate((x4,RModel50.coef_[:,None].reshape(-1,1)),axis=1)\n",
    "x6 = np.concatenate((x5,RModel100.coef_[:,None].reshape(-1,1)),axis=1)\n",
    "x7 = np.concatenate((x6,RModel1000.coef_[:,None].reshape(-1,1)),axis=1)\n",
    "df = pd.DataFrame(x7, columns = (\"Feature names\",\"Rigde C = 0.1 \",\"Rigde C = 1\",\"Rigde C = 5 \",\"Rigde C = 10\",\"Rigde C = 50 \",\"Rigde C = 100\",\"Rigde C = 1000\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we also print the score to show, for this dataset, whether more/less regularization will contribute to the performance.\n",
    "(This is not part of the required answer. Just for better understanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9440559440559441\n",
      "0.958041958041958\n",
      "0.951048951048951\n",
      "0.958041958041958\n",
      "0.958041958041958\n",
      "0.958041958041958\n",
      "0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "print (RModel01.score(X_test, Y_test))\n",
    "print (RModel1.score(X_test, Y_test))\n",
    "print (RModel5.score(X_test, Y_test))\n",
    "print (RModel10.score(X_test, Y_test))\n",
    "print (RModel50.score(X_test, Y_test))\n",
    "print (RModel100.score(X_test, Y_test))\n",
    "print (RModel1000.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What happens to the coefficients as you increase C?\n",
    "\n",
    "When we increase C, we have less regularization. This means that coefficients will have less shrinkage. This way, coefficents increase.\n",
    "\n",
    "- What happens to the flexibility of the model as you increase C?\n",
    "\n",
    "When we increase C, we have less regularization. This means that more coefficients are playing a role in building the model, which makes the model more complex. So flexibility increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3>  Problem #3. Logistic Regression and Unbalanced Datasets  </h3> \n",
    "### [25 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We fit a logistic regression model to predict the probability that an individual will default on his/her credit card balance. We used the total balance (single feature) to fit the model and got the results shown in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-1. </h4> Prediciton with Logistic regression <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Table|Coefficient|Std.error|Z-statistic|P-Value|\n",
    "|:--:|:-------------------------------:|\n",
    "|Intercept|-10.6513|0.3612|-29.5|<0.0001|\n",
    "|balance|0.0055|0.002|24.9|<0.0001|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You can refer to page 14 in the our slides \"Logistic Regression.pdf\" if the above table does not show well.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the parametric model used in logistic regression?\n",
    "- What is the class label of an individual with a balance equals to 15,000 dollar? What is the class label of an individual with balance equals to 800 dollar will not default? (Write a python function which takes two inputs (feature, model_parameters) returns the class labels for the data). default is class 1 and non-default is class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer:\n",
    "\n",
    "### for bullet point 1:\n",
    "\n",
    "numerator = exp($\\beta_0$+$\\beta_1 X$)\n",
    "\n",
    "demominator = exp($\\beta_0$+$\\beta_1 X$) + 1\n",
    "\n",
    "$p(X)$ = numerator/demominator = $exp(\\beta_0+\\beta_1 X)/[1+exp(\\beta_0+\\beta_1 X)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for bullet point 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write your narrative answer here\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def pred_logistic(test, params):        \n",
    "    pred = sigmoid(np.dot(test,params))\n",
    "    pred[pred >= 0.5] = 1\n",
    "    pred[pred < 0.5] = 0\n",
    "    return pred\n",
    "\n",
    "t = np.array([[1,15000],[1,800]])\n",
    "p = np.array([-10.6513, 0.0055])\n",
    "print(pred_logistic(t,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the class label for salary=15000 is default and for salary=800 is non-default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-2. </h4>  <br>\n",
    "\n",
    "The coefficients of logistic regression are obtained by maximizing the likelihood function\n",
    "\n",
    "\\begin{array} \\\\\n",
    "l(\\beta) = \\prod_{i:y_{i}=1} P(y_{i} = 1|x)\\prod_{i{}':y_{{i}'}=0} (1-P(y_{{i}'} = 1|x))\n",
    "\\end{array}\n",
    "Show that maximizing the\n",
    "likelihood function is equivalent to minimizing the cost function $J(\\beta)$, such that.\n",
    "\\begin{array} \\\\\n",
    "J(\\beta) = -\\sum [y_{i} log(P(y_{i} = 1|x)) + (1- y_{i})log(1- P(y_{i} = 1|x))]\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "Here $n$ is the number training examples. Mention one possible method for obtaining the\n",
    "optimal coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maximize:\n",
    "\\begin{array} \\\\\n",
    "l(\\beta) = \\prod_{i:y_{i}=1} P(y_{i} = 1|x)\\prod_{i{}':y_{{i}'}=0} (1-P(y_{{i}'} = 1|x))\n",
    "\\end{array}\n",
    "= maximize:\n",
    "\\begin{array} \\\\\n",
    " \\log(\\prod_{i:y_{i}=1} P(y_{i} = 1|x))+\\log(\\prod_{i{}':y_{{i}'}=0} (1-P(y_{{i}'} = 1|x)))\n",
    "\\end{array}\n",
    "= minimize:\n",
    "\\begin{array} \\\\\n",
    " -[\\log(\\prod_{i:y_{i}=1} P(y_{i} = 1|x))+\\log(\\prod_{i{}':y_{{i}'}=0} (1-P(y_{{i}'} = 1|x)))]\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= minimize:\n",
    "\\begin{array} \\\\\n",
    " -[\\sum_{i:y_{i}=1}\\log( P(y_{i} = 1|x))+\\sum_{i{}':y_{{i}'}=0}\\log( (1-P(y_{{i}'} = 1|x)))]\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= minimize:\n",
    "\\begin{array} \\\\\n",
    " -[\\sum_{i}^{m}[y_i\\log( P(y_{i} = 1|x))+(1-y_i)\\log( 1-P(y_{{i}'} = 1|x))]]\n",
    "\\end{array}\n",
    "\n",
    "Note that the term $(1-y_i)$ will be equal to '1' only when $y_i=0$.\n",
    "Right now this equation has exactly the same form of $J(\\beta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the optimal coefficient $\\beta$, we can use gradient descent.\n",
    "\n",
    "The basic idea is that to get the term of partial derivative of $J(\\beta)$ with respect to the variable $\\beta$, that is: $\\frac{\\partial J(\\beta)}{\\partial \\beta}$. \n",
    "\n",
    "$\\frac{\\partial J(\\beta)}{\\partial \\beta}$ is used to calculate the gradient. Based on learning rate, the gradient and $\\beta$ is updated until the gradient approaches 0. \n",
    "\n",
    "$\\beta$ at this moment can be an optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-3</h4> <br>\n",
    "In a fraud detection system, a QDA classifier‚Äôs confusion matrix is found to be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        |Predicted Class - Not fraud| Predicted Class - fraud|\n",
    "|:--:|:-------------------------------:|\n",
    "|Actual class ‚Äì Not fraud|1200|25|\n",
    "|Actual class ‚Äì fraud|30|7|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is dataset balanced? Why?\n",
    "- Evaluate the overall error rate <br>\n",
    "- Evaluate the precision and the recall <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Answer</h2><br> \n",
    "\n",
    "-Dataset is not balansed. The total number of examples of fraud is much lower than the number of examples of not fraud.\n",
    "\n",
    "\\begin{array} \\\\\n",
    "Error rate = \\frac{25+30}{1200+7+25+30} = \\frac{55}{1262} = 0.043\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "\\begin{array} \\\\\n",
    "Precision  = \\frac{7}{25+7}  = 0.2\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array} \\\\\n",
    "Recall = \\frac{7}{30+7} = 0.189\n",
    "\\end{array}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3>  Problem #4. SVM, Decision Trees, MLP Classification   </h3> \n",
    "### [30 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will use different classification methods, SVM, KNN, Decision Tree and MLP; and find their accuracies using the test data. \n",
    "We will also use the Wisconson **\"breast_cancer\"** dataset.\n",
    "In all of the following subparts, use random_state=0 when you split the dataset into train and test and **standardize** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "  \n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(dataset.data, dataset.target, random_state= 0)\n",
    "\n",
    "# standardize data\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_transformed= scaler.transform(X_train)\n",
    "X_test_transformed= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-1.  Classification with SVM </h4><br>\n",
    "- How does the Radial Basis Function Kernel in SVM measure the similarity between a test point and a training example?\n",
    "- Fit an SVM classifier with a radial basis function kernel, with gamma =0.1, and regularization parameter C set to 10. Use the classifier to predict class labels for the test data. \n",
    "- Calculate the accuracy and confusion matrix on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "In order to measure the similarity between supports vectors and the new observations, Radial Basis Function Kernel uses a Gaussian-like function. It computes the squared Euclidean distance between the observation and training datapoint multiplying to a gamma. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965034965035\n",
      "The confusion matrix of SVM with RBF kernel is \n",
      " [[52  1]\n",
      " [ 4 86]]\n"
     ]
    }
   ],
   "source": [
    "#RBF kernel\n",
    "from sklearn.svm import SVC \n",
    "svm_model_rbf= SVC(kernel = 'rbf', gamma=0.1, C = 10).fit(X_train_transformed, y_train) \n",
    "svm_predictions_rbf = svm_model_rbf.predict(X_test_transformed) \n",
    "# model accuracy for X_test   \n",
    "accuracy_rbf = svm_model_rbf.score(X_test_transformed, y_test) \n",
    "print(accuracy_rbf)\n",
    "# creating a confusion matrix \n",
    "Conf_Mat_rbf = confusion_matrix(y_test, svm_predictions_rbf) \n",
    "print(\"The confusion matrix of SVM with RBF kernel is \\n\", Conf_Mat_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-2.  Classification with Decisin Tree (DT) </h4><br>\n",
    "- In this part, use DT classification method on the training data. Set the maximum depth of the tree to five. Then use the classifier to predict class labels for the test data. Calculate the accuracy and confusion matrix on the test data.\n",
    "- Use Adaboost to combine four decision trees each of max_depth of five. Use random_state=0 in adaboost. Find the test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916083916084\n",
      "[[50  3]\n",
      " [ 9 81]]\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "# training a DescisionTreeClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "dtree_model = DecisionTreeClassifier(max_depth = 5).fit(X_train_transformed, y_train) \n",
    "print(dtree_model.score(X_test_transformed, y_test) )\n",
    "\n",
    "dtree_predictions = dtree_model.predict(X_test_transformed) \n",
    "  \n",
    "# creating a confusion matrix \n",
    "conf_Mat_tree = confusion_matrix(y_test, dtree_predictions) \n",
    "print(conf_Mat_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with AdaBoost is: 0.937062937063\n"
     ]
    }
   ],
   "source": [
    "#Adaboost\n",
    "NumberOfModels=4\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "BoostModel= AdaBoostClassifier(DecisionTreeClassifier(max_depth = 5),n_estimators=NumberOfModels, random_state=0).fit(X_train_transformed,y_train)\n",
    "print(\"Accuracy with AdaBoost is:\", BoostModel.score(X_test_transformed,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-3 </h4>\n",
    "\n",
    "Follow steps to answer questions to build a neural network using MLPClassifier from sklearn.neural_network. \n",
    "- Build a model that has two hidden layers, the first layer has 10 neurons and second layer has 5 neurons. \n",
    "- Use 'relu' activation function, and set the regularization parameter alpha=0.5. \n",
    "- Set max_iter=1000; Set the random_state=0.\n",
    "- Use stochastic gradient descent (sgd) to solve the optimization  problem\n",
    "- Report accuracy, confusion matrix, precision, and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.979020979021\n",
      "Precision: 0.988764044944\n",
      "Recall: 0.977777777778\n",
      "Confusion Matrix: \n",
      "[[52  1]\n",
      " [ 2 88]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "mlpClassifier = MLPClassifier(solver='sgd', max_iter=1000,  activation='relu', random_state=0, hidden_layer_sizes=[10,5], alpha=0.5).fit(X_train_transformed, y_train)\n",
    "predictmlpClassifier= mlpClassifier.predict(X_test_transformed)\n",
    "print(\"Accuracy: \" , mlpClassifier.score(X_test_transformed,y_test))\n",
    "print('Precision:', precision_score(y_test, predictmlpClassifier))\n",
    "print('Recall:', recall_score(y_test, predictmlpClassifier))\n",
    "print(\"Confusion Matrix: \")\n",
    "print( confusion_matrix(y_test, predictmlpClassifier))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4> Problem #4-4 [5 points]</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same setting as problem 4-3, but instead of using two hidden layers, use three hidden layers with 10, 8, 5 hidden neurons respectively.\n",
    "- Find accuracy, precision, recall and confusion matrix. \n",
    "- Comment on the result comparing 4-3 and 4-4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.951048951049\n",
      "Precision: 0.956043956044\n",
      "Recall: 0.966666666667\n",
      "Confusion Matrix: \n",
      "[[49  4]\n",
      " [ 3 87]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "mlpClassifier = MLPClassifier(solver='sgd', max_iter=1000,  activation='relu', random_state=0, hidden_layer_sizes=[10,8,5], alpha=0.5).fit(X_train_transformed, y_train)\n",
    "predictmlpClassifier= mlpClassifier.predict(X_test_transformed)\n",
    "print(\"Accuracy: \" , mlpClassifier.score(X_test_transformed,y_test))\n",
    "print('Precision:', precision_score(y_test, predictmlpClassifier))\n",
    "print('Recall:', recall_score(y_test, predictmlpClassifier))\n",
    "print(\"Confusion Matrix: \")\n",
    "print( confusion_matrix(y_test, predictmlpClassifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the hidden layer, increases the number of the weights in the network that need to be estimated. With the same setting as 4-3, the performance degraded in 4-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
